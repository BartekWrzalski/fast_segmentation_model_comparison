{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ESPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2018 Mehta *et al* [[1]](#references) introduced model ESPNet for fast and efficient semantic segmentation of high resolution images under resource contrains. ESPNet is based on convolutional module efficient spacial pyramid - ESP. When introduced, model was more than 20 times faster and weight almost 180 times less than state of the art model or its time, PSPNet [[2]](#references). \n",
    "\n",
    "Authors based their model on coder-decoder model and convolutional factorization principle. This principle decompose standard convolution to two steps, point-wise convolutions, that use 1x1 kernel size, and spatial pyramid of dilated convolutions. First step helps in reducing amount of computations, and the second part allows network to learn representations from large effective receptive field.\n",
    "\n",
    "<center><img src='images/ESP_1.png'></center>\n",
    "<center>Fig. 1: Convolution Factorization</center>\n",
    "\n",
    "To get rid of artifacts produced by effective receptive field of the ESP module, authors used hierahical feature fusion (HFF). HFF is a fusion that hierarchically adds feature maps obtained using kernels of different dilatation rates using element-wise sum:\n",
    "\n",
    "<center><img src='images/ESP_2.png'></center>\n",
    "<center>Fig. 2: Block diagram of ESP module</center>\n",
    "\n",
    "Authors declare that their module outperfomed other popular modules at a time, such as MobileNet [[3]](#references), ShuffleNet [[4]](#references) or ENet [[5]](#references), achieving 60% mIoU on Cityscapes dataset [[6]](#references) having less parameters. ESPNet has only about 0.4 million parameters which is only about 1.5 MB on size. When it comes to speed, model can process high resolution image at a rate of 112 FPS on NVIDIA Titan X GPU.\n",
    "\n",
    "[[pdf]](https://arxiv.org/pdf/1803.06815)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast-SCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Fast Segmentation Convolutional Neural Network (Fast-SCNN) was proposed by Rudra *et al* [[7]](#references) in 2019.\n",
    "\n",
    "Fast-SCNN was inspired by the two-branch architectures [[8, 9]](#references) and encoder-decoder network with skip connections [[10, 11]](#references)\n",
    "\n",
    "<center><img src='images/Fast-SCNN_1.png'></center>\n",
    "<center>Fig. 3: Comparison of encoder-decoder and two-branch architectures with Fast-SCNN module</center>\n",
    "\n",
    "Semantic segmentation methods that run in real-time at that time were based on networks with two branches, each operating on a different resolution level. They were learning global information from low resolution versions of the input image, and employed shallow networks at full input resolution to refine the precision of the segmentation results. Since input resolution and network depth are main factors for runtime, these two-branch approaches allow for real-time computation. \n",
    "\n",
    "It is known that the first few layers of DCNNs extract the low-level features, such as edges and corners. Therefore, instead of employing a two-branch approach, authors introduced learning to downsample, which shares feature computation between the low and high-level branch in a shallow network block, instead of separated computation in two-branches architecture. This module is using 3 layers, one standars convolutional layer and two depthwise separable convolutional layers, and is equivalent to spatial path used in two-branch architecture.\n",
    "\n",
    "Module achives 68.0% mIoU on Cityscapes database [[6]](#references) having 1.11 million parameters (~4.5 MB) and with speed of 123 FPS\n",
    "\n",
    "[[pdf]](https://arxiv.org/pdf/1902.04502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ShuffleNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2018, Ma *et al* [[12]](#references) published paper that derives several practical guidelines for efficient network edsign. In the same paper authors introduced new model, inspired by ShuffleNet [[13]](#references), thus called ShuffleNetV2.\n",
    "\n",
    "<center><img src='images/ShuffleNet_1.png'></center>\n",
    "<center>Fig. 4: Building blocks of ShuffleNet and ShuffleNetV2. (a): basic ShuffleNet unit; (b): ShuffleNet unit for spatial down sampling; (c) ShuffleNetV2 base unit; (d): ShuffleNetV2 unit for spatial down sampling</center>\n",
    "\n",
    "In the paper, the authors shows, that number eof computations do not match speed of module. What is important is Memory Access Cost (MAC), and that cost is non-negliable, especially for smaller modules. To increase spped of module and decrease constful operations, authors introduce following guidelines:\n",
    "1. Channels width on input and output should be the same\n",
    "2. Group convolutions are more constful than normal convolutions\n",
    "3. Degree of fragmentation should be reduced\n",
    "4. Amount of element-wise operations should be reduced\n",
    "\n",
    "Result of usage of this guildelines can be seen on Figure 4. We see that authors changes base building unit ((a) -> (c)) by changing group convolutions to regular convolutions, unifying the number of input and ouput channels and removing 'Add' operator. Same goes for unit for spatial down-sampling.\n",
    "\n",
    "In their experiment, authors proved speed of their module to be 58% faster that MobileNetV2 [[14]](#references) and 63% faster than ShuffleNet [[13]](#references). Depending on module size its speed can vary from 115 to almost 250 FPS.\n",
    "\n",
    "[[pdf]](https://arxiv.org/pdf/1807.11164)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiSeNetV2\n",
    "In 2020 Yu *et al* [[15]](#references) proposed module BiSeNetV2 (Bilateral Segmentation Network) based on previously introduced by same authors, BiSeNet [[16]](#references). Key feature of this architecture involved two branches, Detail branch and Semantic Branch.\n",
    "\n",
    "Detail branch was designed to capture spatial details, that means low-level information. Key concept of this branch is to to use shallow layers and wide channels. Morechannles are required to encode detailed information.\n",
    "\n",
    "In parallel, Semantic branch was designed to capture high-level information. This branch has low channel capacity, modelled by ratio $\\lambda$ (authors used $\\lambda$ = 1/4) channels of Detail branch, which makes it lighter.\n",
    "\n",
    "At the end, authors used Aggregation Layer, to merge representations from two branches.\n",
    "\n",
    "Concept is generci, and can be implemented with defferent convolutional models and many designs, as long as it follows those three parts: Detail Branch, Semantic Branch and Agregation Layer.\n",
    "\n",
    "<center><img src='images/BiSeNet_1.png'></center>\n",
    "<center>Fig. 5: Bilateral Segmentation Network</center>\n",
    "_\n",
    "<center><img src='images/BiSeNet_2.png'></center>\n",
    "<center>Fig. 6: Visual representation Network results</center>\n",
    "\n",
    "On experiments, authors achive ~73% mIoU and 156 FPS on Cityscapes dataset with GPU Nvidia GeForce 1080 Ti.\n",
    "\n",
    "[[pdf]](https://arxiv.org/pdf/2004.02147)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileSAM\n",
    "In 2023 Kirillov *et al* [[17]](#references) shows the world Segment Anything Model (SAM). Model developed by Meta AI Reasearch is considered state of the art at the time.\n",
    "\n",
    "Model was based on transformers [[18, 19, 20, 21]](#references) for both encoder and decoder. Model was trained on multiple datasets, and images could have multiple masks. By adding another modality, model was trained to segmetn image based on text input. This is why model was called 'Segment Anything' and not everything.\n",
    "\n",
    "Model was not quite fast, due to very heavy encoder. So in 2023 we get 2 models that made SAM faster - FastSAM [[22]](#references) and MobileSAM [[23]](#references). In MobileSAM, authors showed that their model is 60 times smaller than SAM and perform on par with it. Additionally, MobileSAM is 7 times smaller and 5 times faster than than FastSAM, thus I focused on this model. Model has 5.78 million parameters (~23MB) and ourperformes Fast-SAM in terms of accuracy up to 3 times in mIoU metric.\n",
    "\n",
    "[[pdf]](https://arxiv.org/pdf/2306.14289)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNetV3 + DeepLabV3\n",
    "MobileNetV3 was introduced by Howard *et al* [[24]](#references) in 2019, based on units introduced in MobileNetV1 [[25]](#references), MobileNetV2 [[14]](#references) and MnasNet [[26]](#references).\n",
    "\n",
    "Authors search for architecture using Reinforcement Learning introduced in [[27]](#references) and NetAdapt [[28]](#references). As base architecture, authors reuse MnasNet-A1 and fine-tune individual layers with NetAdapt. That means, authors generate set of new proposal that was better than model in previous step by some metric, then populate and fine-tune each proposal. At the end, best proposal was choosen as base model for next step, and so on until target latency was reached.\n",
    "\n",
    "On top of that, authors used modified ReLU to closen function to sigmoid function. The same goes for swish [[29, 30, 31]](#references) function. Usage of linear functions in replacement of non-linear ones, allowed to improve accuracy without increase in computional cost associated with using non-linear functions.\n",
    "\n",
    "<center><img src='images/MobileNet_1.png'></center>\n",
    "<center>Fig. 6: Imapct of individual components in the development of MobileNetV3</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. S. Mehta, M. Rastegari, A. Caspi, L. Shapiro, H. Hajishirzi. ESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation (2018)\n",
    "2. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J. Pyramid scene parsing network. In: CVPR. (2017)\n",
    "3. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. (2017)\n",
    "4. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolutional neural network for mobile devices. In: CVPR. (2018)\n",
    "5. A. Paszke, A. Chaurasia, S. Kim, E. Culurciello. ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation (2016)\n",
    "6. M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic urban scene understanding. (2016)\n",
    "7. R. P K Poudel, S. Liwicki, R. Cipolla. Fast-SCNN: Fast Semantic Segmentation Network\n",
    "8. D. Mazzini. Guided Upsampling Network for Real-Time Semantic Segmentation. In BMVC, 2018\n",
    "9. C. Yu, J. Wang, C. Peng, C. Gao, G. Yu, and N. Sang.Bisenet: Bilateral segmentation network for real-time semantic segmentation. In ECCV, 2018\n",
    "10. O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. In MICCAI, (2015)\n",
    "11. E. Shelhamer, J. Long, and T. Darrell. Fully convolutional networks for semantic segmentation. PAMI, (2016)\n",
    "12. N. Ma, X. Zhang, H. Zheng, J. Sun. ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design (2018)\n",
    "13. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolutional neural network for mobile devices. (2017)\n",
    "14. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation (2018)\n",
    "15. Yu C, Gao C, Wang J, Yu G, Shen C, Sang N: BiSeNet V2: Bilateral Network with Guided Aggregation for Real-time Semantic Segmentation (2020)\n",
    "16. Yu C, Wang J, Peng C, Gao C, Yu G, Sang N: Bisenet: Bilateral segmentation network for real-time semantic segmentation (2018)\n",
    "17. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. (2023)\n",
    "18. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with Transformers. ECCV, (2020)\n",
    "19. Bowen Cheng, Alex Schwing, and Alexander Kirillov. Perpixel classification is not all you need for semantic segmentation. NeurIPS, (2021)\n",
    "20. Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. SimpleClick: Interactive image segmentation with simple vision transformers. (2022)\n",
    "21. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. (2017)\n",
    "22. Xu Zhao, Wenchao Ding, Yinglong Du, Tao Yu, Min Li, Ming Tang, Jinqiao Wang: Fast Segment Anything (2023)\n",
    "23. Zhang Chaoning, Han Dongshen, Qiao Yu, Jung Uk Kim, Bae Sung-Ho, Lee Seungkyu, Choong Seon Hong: Faster Segment Anything: Towards Lightweight SAM for Mobile Applications (2023)\n",
    "24. Andrew G. Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yakun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V.Le, Hartwig Adam: Searching for MobileNetV3 (2019)\n",
    "25. Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. CoRR (2017)\n",
    "26. Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. (2018)\n",
    "27. Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. (2016)\n",
    "28. Tien-Ju Yang, Andrew G. Howard, Bo Chen, Xiao Zhang, Alec Go, Mark Sandler, Vivienne Sze, and Hartwig Adam. Netadapt: Platform-aware neural network adaptation for mobile applications. In ECCV, (2018)\n",
    "29. Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoidweighted linear units for neural network function approximation in reinforcement learning. (2017)\n",
    "30. Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. (2016)\n",
    "31. Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions (2017)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
